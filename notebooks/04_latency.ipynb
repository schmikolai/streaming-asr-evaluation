{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14fb23c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULT_DIR = \"../out/2025-05-05_13-29-23_librispeech-pc-test-clean_large-v3-turbo\"\n",
    "# RESULT_DIR = \"/home/niko/Downloads/results\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f279a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from src.eval.SampleResult import SampleResult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75316dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir(RESULT_DIR)\n",
    "files = [f for f in files if f.endswith(\"final.json\")]\n",
    "\n",
    "# get ids from the first part of the filename separated by \"_\"\n",
    "file_ids = [f.split(\"_\")[0] for f in files]\n",
    "\n",
    "samples = [SampleResult.load_by_id(RESULT_DIR, file_id) for file_id in file_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908097a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe97ad68",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(by=[\"alignment_sequence\", \"temporal_alignment_tolerance\"], inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4698008e",
   "metadata": {},
   "source": [
    "# Evaluation Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc038b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ALIGNMENT_SEQUENCE = \"final\"\n",
    "TEMPORAL_ALIGNMENT_TOLERANCE = 0.1\n",
    "ALIGNMENT_WORD_NORMALIZATION = True     # No difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9520e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval = pd.Series()\n",
    "eval[\"num_samples\"] = len(samples)\n",
    "eval[\"alignment_sequence\"] = ALIGNMENT_SEQUENCE\n",
    "eval[\"alignment_word_normalization\"] = ALIGNMENT_WORD_NORMALIZATION\n",
    "eval[\"temporal_alignment_tolerance\"] = TEMPORAL_ALIGNMENT_TOLERANCE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e229d4",
   "metadata": {},
   "source": [
    "## Alignments\n",
    "\n",
    "Build the alignments relative to either the `final` sequence of the online, or the `baseline` with offline transcription."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a7c71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "_alignment_sequences = [s.final if ALIGNMENT_SEQUENCE == \"final\" else s.baseline for s in samples]\n",
    "\n",
    "for sample in samples:\n",
    "    sample.build_alignments(normalize_words=ALIGNMENT_WORD_NORMALIZATION,\n",
    "                            align_to=ALIGNMENT_SEQUENCE,\n",
    "                            temporal_tolerance=TEMPORAL_ALIGNMENT_TOLERANCE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d0c91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_df = pd.DataFrame([{\n",
    "    \"id\": sample.sample_id,\n",
    "    \"sample\": sample,\n",
    "    \"word_count_final\": len(sample.final),\n",
    "    \"word_count_baseline\": len(sample.baseline),\n",
    "    \"word_count_transcript\": len(sample.transcript),\n",
    "} for sample in samples])\n",
    "samples_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed182e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_partial_words(sample: SampleResult) -> int:\n",
    "    partial_words = [len(partial.result) for partial in sample.partials]\n",
    "    return sum(partial_words)\n",
    "\n",
    "def unaligned_partial_words(sample: SampleResult) -> int:\n",
    "    unalignments = [len(alignements.unalignments) for alignements in sample.alignments]\n",
    "    return sum(unalignments)\n",
    "\n",
    "samples_df[\"unaligned_partial_count\"] = samples_df[\"sample\"].apply(unaligned_partial_words)\n",
    "samples_df[\"partial_count\"] = samples_df[\"sample\"].apply(total_partial_words)\n",
    "\n",
    "eval[\"total_partial_count\"] = samples_df[\"partial_count\"].sum().item()\n",
    "eval[\"unaligned_partial_count\"] = samples_df[\"unaligned_partial_count\"].sum().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe03f12",
   "metadata": {},
   "source": [
    "## Word first correct\n",
    "\n",
    "The word first correct (wfc) is defined as the first correct occurence of a word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4091b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.eval.metrics.word_first_correct import word_first_correct_response\n",
    "wfc = [[word_first_correct_response(_alignment_sequences[j], s.partials, i, s.alignments) for i in range(len(_alignment_sequences[j]))] for j, s in enumerate(samples)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db18a175",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_wfc = np.concat(wfc)\n",
    "wfc_latency = np.array([r[\"latency\"] for r in total_wfc if r is not None])\n",
    "none_wfc_count = len(total_wfc) - len(wfc_latency)\n",
    "print(\"None values in wfc: \", none_wfc_count, f\"({none_wfc_count / len(total_wfc) * 100:.2f}%)\")\n",
    "eval[\"wfc_none_count\"] = none_wfc_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69340de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Min:\", np.min(wfc_latency))\n",
    "print(\"Max:\", np.max(wfc_latency))\n",
    "print(\"Mean:\", np.mean(wfc_latency))\n",
    "print(\"Median:\", np.median(wfc_latency))\n",
    "print(\"Std:\", np.std(wfc_latency))\n",
    "print(\"P95:\", np.percentile(wfc_latency, 95))\n",
    "print(\"P99:\", np.percentile(wfc_latency, 99))\n",
    "\n",
    "eval[\"wfc_latency_min\"] = np.min(wfc_latency).item()\n",
    "eval[\"wfc_latency_max\"] = np.max(wfc_latency).item()\n",
    "eval[\"wfc_latency_mean\"] = np.mean(wfc_latency).item()\n",
    "eval[\"wfc_latency_std\"] = np.std(wfc_latency).item()\n",
    "eval[\"wfc_latency_median\"] = np.median(wfc_latency).item()\n",
    "eval[\"wfc_latency_negative_count\"] = len([x for x in wfc_latency if x < 0])\n",
    "eval[\"wfc_latency_p95\"] = np.percentile(wfc_latency, 95).item()\n",
    "eval[\"wfc_latency_p99\"] = np.percentile(wfc_latency, 99).item()\n",
    "eval[\"wfc_latency_count\"] = len(wfc_latency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5408d66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(wfc_latency,\n",
    "         bins=100,\n",
    "         range=(math.floor(np.percentile(wfc_latency, 1)), math.ceil(np.percentile(wfc_latency, 99).item()))\n",
    "         )\n",
    "plt.xlabel(\"Latency (s)\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(f\"Word First Correct (WFC) Latency Distribution, {ALIGNMENT_SEQUENCE} alignment\")\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671edb42",
   "metadata": {},
   "source": [
    "## Word first final\n",
    "\n",
    "The word first final (wff) is defined as the first occurence after which a word will never change to a different prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40cff5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.eval.metrics.word_first_final import word_first_final_response\n",
    "wff = [[word_first_final_response(_alignment_sequences[j], s.partials, i, s.alignments) for i in range(len(_alignment_sequences[j]))] for j, s in enumerate(samples)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ccad6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_wff = np.concat(wff)\n",
    "wff_latency = np.array([r[\"latency\"] for r in total_wff if r is not None])\n",
    "none_wff_count = len(total_wff) - len(wff_latency)\n",
    "print(\"None values in wff: \", none_wff_count, f\"({none_wff_count / len(total_wff) * 100:.2f}%)\")\n",
    "eval[\"wff_none_count\"] = none_wff_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c2ae9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Min:\", np.min(wff_latency))\n",
    "print(\"Max:\", np.max(wff_latency))\n",
    "print(\"Mean:\", np.mean(wff_latency))\n",
    "print(\"Median:\", np.median(wff_latency))\n",
    "print(\"Std:\", np.std(wff_latency))\n",
    "print(\"P95:\", np.percentile(wff_latency, 95))\n",
    "print(\"P99:\", np.percentile(wff_latency, 99))\n",
    "\n",
    "eval[\"wff_latency_min\"] = np.min(wff_latency).item()\n",
    "eval[\"wff_latency_max\"] = np.max(wff_latency).item()\n",
    "eval[\"wff_latency_mean\"] = np.mean(wff_latency).item()\n",
    "eval[\"wff_latency_std\"] = np.std(wff_latency).item()\n",
    "eval[\"wff_latency_median\"] = np.median(wff_latency).item()\n",
    "eval[\"wff_latency_negative_count\"] = len([x for x in wff_latency if x < 0])\n",
    "eval[\"wff_latency_p95\"] = np.percentile(wff_latency, 95).item()\n",
    "eval[\"wff_latency_p99\"] = np.percentile(wff_latency, 99).item()\n",
    "eval[\"wff_latency_count\"] = len(wff_latency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98965ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(wff_latency, bins=100, range=(math.floor(np.percentile(wff_latency, 1)), math.ceil(np.percentile(wff_latency, 99).item())))\n",
    "plt.xlabel(\"Latency (s)\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(f\"Word First Final (WFF) Latency Distribution, {ALIGNMENT_SEQUENCE} alignment\")\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6919554c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df, eval.to_frame().T], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed81eea",
   "metadata": {},
   "source": [
    "## Visualize Alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba560ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "id = \"7127-75946-0000\"\n",
    "sample = next(s for s in samples if s.sample_id == id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a47570f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "DRAW_CONFIRMED_ALIGNMENTS = False\n",
    "\n",
    "_alignment_sequence = sample.final if ALIGNMENT_SEQUENCE == \"final\" else sample.baseline\n",
    "\n",
    "window_length = 30\n",
    "window_start = 473\n",
    "window_end = window_start + window_length\n",
    "\n",
    "words = _alignment_sequence[window_start:window_end]\n",
    "\n",
    "window_start_t = words[0].start\n",
    "window_end_t = words[-1].end\n",
    "\n",
    "# Filter predictions and keep original indices\n",
    "indexed_predictions = [\n",
    "    (i, p) for i, p in enumerate(sample.partials)\n",
    "    if window_start_t <= p.window[1] and (p.observation_time <= window_end_t or p.window[1] <= window_end_t)\n",
    "]\n",
    "indexed_predictions = indexed_predictions[::-1]  # Reverse for display\n",
    "\n",
    "# Final word mapping (global indices to x/y coordinates)\n",
    "final_word_positions = {\n",
    "    i: {\n",
    "        \"x\": (word.start + word.end) / 2,\n",
    "        \"y\": len(indexed_predictions) + 0.25  # fixed y for final row\n",
    "    }\n",
    "    for i, word in enumerate(_alignment_sequence)\n",
    "}\n",
    "\n",
    "# Determine max observation time for x-axis limit\n",
    "max_obs_time = max(window_end_t, max(p.observation_time for _, p in indexed_predictions))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(30, 8))\n",
    "\n",
    "for row_index, (original_index, partial) in enumerate(indexed_predictions):\n",
    "    win_start, win_end = partial.window\n",
    "    obs_time = partial.observation_time\n",
    "\n",
    "    # Green prediction window\n",
    "    ax.add_patch(Rectangle((win_start, row_index), win_end - win_start, 1, color=\"green\", alpha=0.3))\n",
    "\n",
    "    marker_width = 0.01\n",
    "    ax.add_patch(Rectangle((win_end - marker_width / 2, row_index), marker_width, 1, color=\"green\", alpha=0.9))\n",
    "\n",
    "    # Red delay box\n",
    "    if obs_time > win_end:\n",
    "        delay = obs_time - win_end\n",
    "        ax.add_patch(Rectangle(\n",
    "            (win_end, row_index), delay, 1,\n",
    "            color=\"red\", alpha=0.2, hatch='////', linewidth=0.5, fill=True\n",
    "        ))\n",
    "        # Delay label\n",
    "        ax.text(win_end + delay / 2, row_index + 0.5, f\"{delay:.2f}s\", ha=\"center\", va=\"center\", fontsize=10, color=\"red\")\n",
    "\n",
    "    ax.add_patch(Rectangle((obs_time - marker_width / 2, row_index), marker_width, 1, color=\"red\", alpha=0.9))\n",
    "\n",
    "    # Word boxes\n",
    "    for word_index, word in enumerate(partial.result):\n",
    "        if window_start_t <= word.start <= window_end_t:\n",
    "            ax.add_patch(Rectangle(\n",
    "                (word.start, row_index + 0.25), word.end - word.start, 0.5,\n",
    "                edgecolor=\"darkgreen\", facecolor=\"none\", linewidth=1\n",
    "            ))\n",
    "            ax.text((word.start + word.end) / 2, row_index + 0.5, word.word,\n",
    "                    ha=\"center\", va=\"center\", fontsize=9, color=\"darkgreen\")\n",
    "            \n",
    "            # Draw alignment line\n",
    "            wa = None\n",
    "            accepted = False\n",
    "            if DRAW_CONFIRMED_ALIGNMENTS:\n",
    "                for _wa in sample.alignments[original_index].confirmed_alignments:\n",
    "                    if _wa.partial_word_index == word_index:\n",
    "                        wa = _wa\n",
    "                        break\n",
    "            for _wa in sample.alignments[original_index].accepted_alignments:\n",
    "                if _wa.partial_word_index == word_index:\n",
    "                    wa = _wa\n",
    "                    accepted = True\n",
    "                    break\n",
    "            if wa is None and not DRAW_CONFIRMED_ALIGNMENTS:\n",
    "                for _wa in sample.alignments[original_index].potential_alignments:\n",
    "                    if _wa.partial_word_index == word_index:\n",
    "                        wa = _wa\n",
    "                        break\n",
    "            if wa is not None:\n",
    "                if wa.final_word_index in final_word_positions:\n",
    "                    x0 = (word.start + word.end) / 2\n",
    "                    y0 = row_index + 0.75\n",
    "                    x1 = final_word_positions[wa.final_word_index][\"x\"]\n",
    "                    y1 = final_word_positions[wa.final_word_index][\"y\"]\n",
    "                    color = \"blue\" if DRAW_CONFIRMED_ALIGNMENTS else \"lightgreen\" if accepted else \"orange\"\n",
    "                    ax.plot([x0, x1], [y0, y1], color=color, linestyle=\"dashed\", linewidth=1)\n",
    "            else:\n",
    "                ax.plot((word.start + word.end) / 2, row_index + 0.85, \"o\", color=\"red\", markersize=3, alpha=0.5)\n",
    "            \n",
    "\n",
    "# Draw top word layer (blue)\n",
    "word_y = len(indexed_predictions)\n",
    "for i, word in enumerate(words):\n",
    "    start = word.start\n",
    "    end = word.end\n",
    "    ax.add_patch(Rectangle((start, word_y), end - start, 1, color=\"blue\", alpha=0.5))\n",
    "    ax.text((start + end) / 2, word_y + 0.5, word.word, ha=\"center\", va=\"center\", fontsize=10)\n",
    "    ax.text((start + end) / 2, word_y + 1.3, f\"{window_start + i}\", ha=\"center\", va=\"center\", fontsize=10, color=\"black\")\n",
    "    ax.axvline(start, color=\"black\", linestyle=\"--\", linewidth=0.5, alpha=0.5)\n",
    "    ax.axvline(end, color=\"black\", linestyle=\"--\", linewidth=0.5, alpha=0.5)\n",
    "\n",
    "window_end_t = max_obs_time + 0.5\n",
    "\n",
    "# Axes setup\n",
    "ax.set_xlim(window_start_t, window_end_t)\n",
    "ax.set_ylim(0, word_y + 2)\n",
    "ax.set_yticks([i + 0.5 for i in range(len(indexed_predictions))])\n",
    "ax.set_yticklabels([str(idx) for idx, _ in indexed_predictions])\n",
    "ax.xaxis.tick_top()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f033e5ad",
   "metadata": {},
   "source": [
    "# Automated alignment evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "id": "0c2c254c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "id": "452569bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "ALIGNMENT_SEQUENCES = [\"final\", \"baseline\"]\n",
    "TEMPORAL_ALIGNMENT_TOLERANCES = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5194dca5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [02:51<00:00, 28.58s/it]\n",
      "100%|██████████| 6/6 [03:33<00:00, 35.66s/it]]\n",
      "100%|██████████| 2/2 [06:25<00:00, 192.72s/it]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'alignment_sequence'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[32m/var/folders/p7/4nxjn99934d1y7rk0mh96_300000gn/T/ipykernel_98815/4114657625.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     22\u001b[39m         eval[\u001b[33m\"wff_latency\"\u001b[39m] = np.array([r[\u001b[33m\"latency\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;28;01min\u001b[39;00m total_wff \u001b[38;5;28;01mif\u001b[39;00m r \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m])\n\u001b[32m     23\u001b[39m \n\u001b[32m     24\u001b[39m         metrics_df = pd.concat([metrics_df, eval.to_frame().T], ignore_index=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     25\u001b[39m \n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m metrics_df.sort_values(by=[\u001b[33m\"alignment_sequence\"\u001b[39m, \u001b[33m\"temporal_alignment_tolerance\"\u001b[39m], inplace=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     27\u001b[39m metrics_df.info()\n",
      "\u001b[32m~/miniconda3/envs/asr/lib/python3.11/site-packages/pandas/core/frame.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, by, axis, ascending, inplace, kind, na_position, ignore_index, key)\u001b[39m\n\u001b[32m   7168\u001b[39m                 \u001b[33mf\"Length of ascending ({len(ascending)})\"\u001b[39m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m   7169\u001b[39m                 \u001b[33mf\" != length of by ({len(by)})\"\u001b[39m\n\u001b[32m   7170\u001b[39m             )\n\u001b[32m   7171\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m len(by) > \u001b[32m1\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m7172\u001b[39m             keys = [self._get_label_or_level_values(x, axis=axis) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;28;01min\u001b[39;00m by]\n\u001b[32m   7173\u001b[39m \n\u001b[32m   7174\u001b[39m             \u001b[38;5;66;03m# need to rewrap columns in Series to apply key function\u001b[39;00m\n\u001b[32m   7175\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[32m~/miniconda3/envs/asr/lib/python3.11/site-packages/pandas/core/frame.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m7172\u001b[39m         ...     key=\u001b[38;5;28;01mlambda\u001b[39;00m x: np.argsort(index_natsorted(df[\u001b[33m\"time\"\u001b[39m]))\n",
      "\u001b[32m~/miniconda3/envs/asr/lib/python3.11/site-packages/pandas/core/generic.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, key, axis)\u001b[39m\n\u001b[32m   1907\u001b[39m             values = self.xs(key, axis=other_axes[\u001b[32m0\u001b[39m])._values\n\u001b[32m   1908\u001b[39m         \u001b[38;5;28;01melif\u001b[39;00m self._is_level_reference(key, axis=axis):\n\u001b[32m   1909\u001b[39m             values = self.axes[axis].get_level_values(key)._values\n\u001b[32m   1910\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1911\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m KeyError(key)\n\u001b[32m   1912\u001b[39m \n\u001b[32m   1913\u001b[39m         \u001b[38;5;66;03m# Check for duplicates\u001b[39;00m\n\u001b[32m   1914\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m values.ndim > \u001b[32m1\u001b[39m:\n",
      "\u001b[31mKeyError\u001b[39m: 'alignment_sequence'"
     ]
    }
   ],
   "source": [
    "metrics_df = pd.DataFrame()\n",
    "\n",
    "for ALIGNMENT_SEQUENCE in ALIGNMENT_SEQUENCES:\n",
    "    print(f\"Evaluating {ALIGNMENT_SEQUENCE} alignments\")\n",
    "    for TEMPORAL_ALIGNMENT_TOLERANCE in tqdm(TEMPORAL_ALIGNMENT_TOLERANCES):\n",
    "        _alignment_sequences = [s.final if ALIGNMENT_SEQUENCE == \"final\" else s.baseline for s in samples]\n",
    "\n",
    "        for sample in samples:\n",
    "            sample.build_alignments(normalize_words=ALIGNMENT_WORD_NORMALIZATION,\n",
    "                                    align_to=ALIGNMENT_SEQUENCE,\n",
    "                                    temporal_tolerance=TEMPORAL_ALIGNMENT_TOLERANCE)\n",
    "        \n",
    "        eval = pd.Series()\n",
    "        eval[\"alignment_sequence\"] = ALIGNMENT_SEQUENCE\n",
    "        eval[\"temporal_alignment_tolerance\"] = TEMPORAL_ALIGNMENT_TOLERANCE\n",
    "\n",
    "        eval[\"unaligned_partial_words\"] = sum([unaligned_partial_words(s) for s in samples])\n",
    "\n",
    "        wfc = [[word_first_correct_response(_alignment_sequences[j], s.partials, i, s.alignments) for i in range(len(_alignment_sequences[j]))] for j, s in enumerate(samples)]\n",
    "        total_wfc = np.concat(wfc)\n",
    "        eval[\"wfc_latency\"] = np.array([r[\"latency\"] for r in total_wfc if r is not None])\n",
    "\n",
    "        wff = [[word_first_final_response(_alignment_sequences[j], s.partials, i, s.alignments) for i in range(len(_alignment_sequences[j]))] for j, s in enumerate(samples)]\n",
    "        total_wff = np.concat(wff)\n",
    "        eval[\"wff_latency\"] = np.array([r[\"latency\"] for r in total_wff if r is not None])\n",
    "\n",
    "        metrics_df = pd.concat([metrics_df, eval.to_frame().T], ignore_index=True)\n",
    "\n",
    "metrics_df.sort_values(by=[\"alignment_sequence\", \"temporal_alignment_tolerance\"], inplace=True)\n",
    "metrics_df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "id": "d8a1a708",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unaligned_partial_words</th>\n",
       "      <th>wfc_latency</th>\n",
       "      <th>wff_latency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5600</td>\n",
       "      <td>[1.041731370911002, 0.7617313709110021, 11.388...</td>\n",
       "      <td>[15.973553719297051, 15.693553719297052, 15.57...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9021</td>\n",
       "      <td>[2.121731370911002, 0.6341608881205321, 0.4941...</td>\n",
       "      <td>[2.121731370911002, 0.6341608881205321, 0.4941...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8228</td>\n",
       "      <td>[2.121731370911002, 0.6341608881205321, 0.4941...</td>\n",
       "      <td>[2.121731370911002, 0.6341608881205321, 0.4941...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7712</td>\n",
       "      <td>[1.114160888120532, 0.6341608881205321, 0.4941...</td>\n",
       "      <td>[1.114160888120532, 0.6341608881205321, 0.4941...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7335</td>\n",
       "      <td>[1.114160888120532, 0.6341608881205321, 0.4941...</td>\n",
       "      <td>[1.114160888120532, 0.6341608881205321, 0.4941...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7133</td>\n",
       "      <td>[1.114160888120532, 0.6341608881205321, 0.4941...</td>\n",
       "      <td>[1.114160888120532, 0.6341608881205321, 0.4941...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2729</td>\n",
       "      <td>[1.035239720568061, 0.7952397205680608, 0.6552...</td>\n",
       "      <td>[11.867625450566411, 16.74449127547443, 16.604...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6813</td>\n",
       "      <td>[2.121731370911002, 0.6341608881205321, 0.4941...</td>\n",
       "      <td>[2.121731370911002, 0.6341608881205321, 0.4941...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5476</td>\n",
       "      <td>[2.121731370911002, 0.6341608881205321, 0.4941...</td>\n",
       "      <td>[2.121731370911002, 0.6341608881205321, 0.4941...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4715</td>\n",
       "      <td>[1.114160888120532, 0.6341608881205321, 0.4941...</td>\n",
       "      <td>[1.114160888120532, 0.6341608881205321, 0.4941...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>4217</td>\n",
       "      <td>[1.114160888120532, 0.6341608881205321, 0.4941...</td>\n",
       "      <td>[1.114160888120532, 0.6341608881205321, 0.4941...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>3852</td>\n",
       "      <td>[1.114160888120532, 0.6341608881205321, 0.4941...</td>\n",
       "      <td>[1.114160888120532, 0.6341608881205321, 0.4941...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   unaligned_partial_words                                        wfc_latency  \\\n",
       "0                     5600  [1.041731370911002, 0.7617313709110021, 11.388...   \n",
       "1                     9021  [2.121731370911002, 0.6341608881205321, 0.4941...   \n",
       "2                     8228  [2.121731370911002, 0.6341608881205321, 0.4941...   \n",
       "3                     7712  [1.114160888120532, 0.6341608881205321, 0.4941...   \n",
       "4                     7335  [1.114160888120532, 0.6341608881205321, 0.4941...   \n",
       "5                     7133  [1.114160888120532, 0.6341608881205321, 0.4941...   \n",
       "6                     2729  [1.035239720568061, 0.7952397205680608, 0.6552...   \n",
       "7                     6813  [2.121731370911002, 0.6341608881205321, 0.4941...   \n",
       "8                     5476  [2.121731370911002, 0.6341608881205321, 0.4941...   \n",
       "9                     4715  [1.114160888120532, 0.6341608881205321, 0.4941...   \n",
       "10                    4217  [1.114160888120532, 0.6341608881205321, 0.4941...   \n",
       "11                    3852  [1.114160888120532, 0.6341608881205321, 0.4941...   \n",
       "\n",
       "                                          wff_latency  \n",
       "0   [15.973553719297051, 15.693553719297052, 15.57...  \n",
       "1   [2.121731370911002, 0.6341608881205321, 0.4941...  \n",
       "2   [2.121731370911002, 0.6341608881205321, 0.4941...  \n",
       "3   [1.114160888120532, 0.6341608881205321, 0.4941...  \n",
       "4   [1.114160888120532, 0.6341608881205321, 0.4941...  \n",
       "5   [1.114160888120532, 0.6341608881205321, 0.4941...  \n",
       "6   [11.867625450566411, 16.74449127547443, 16.604...  \n",
       "7   [2.121731370911002, 0.6341608881205321, 0.4941...  \n",
       "8   [2.121731370911002, 0.6341608881205321, 0.4941...  \n",
       "9   [1.114160888120532, 0.6341608881205321, 0.4941...  \n",
       "10  [1.114160888120532, 0.6341608881205321, 0.4941...  \n",
       "11  [1.114160888120532, 0.6341608881205321, 0.4941...  "
      ]
     },
     "execution_count": 421,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "asr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
